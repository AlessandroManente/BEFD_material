boost.movies.1=gbm(vote_average ~ .-vote_classes, data=dati.train,
distribution="gaussian", n.trees=best, interaction.depth=4)
summary(boost.movies.1)
# AIC(boost.movies.1)
#Prediction on test set
p.boost=predict(boost.movies.1, newdata=dati.test, n.trees=best)
dev.boost <- sum((p.boost-dati.test$vote_average)^2)
#To compare with linear model and GAM. GBM is the best model considering deviance.
dev.boost
#Default parameters
mai.old<-par()$mai
mai.old
#New parameters equal to old parameters
mai.new<-mai.old
#Substitute parameter relative to left space
mai.new[2] <- 2.1
mai.new
#Modify graphical parameters
par(mai=mai.new)
#las=1 horizontal names on y
summary(boost.movies.1, las=1)
#cBar sets how many variables to draw
summary(boost.movies.1, las=1, cBar=10)
#Back to old parameters
par(mai=mai.old)
#Partial dependence plots
#Univariate
#Budget: we see the same relation also in linear and GAM model, this is a consistent result
plot(boost.movies.1, i.var=1, n.trees = best)
plot(boost.movies.1, i.var=2, n.trees = best)
plot(boost.movies.1, i.var=5, n.trees = best)
plot(boost.movies.1, i.var=3, n.trees = best)
plot(boost.movies.1, i.var=6, n.trees = best)
#Qualitative variable
plot(boost.movies.1, i=23, n.trees = best)
#No effect
plot(boost.movies.1, i=17, n.trees = best)
#Bivariate
plot(boost.movies.1, i.var=c(1,5), n.trees = best)
#REMEMBER: a PDP indicates what is the relation but after having accounted for the presence of
#          all the variables within the model, very different from the GAM models: we have a pure
#          relationship (net relationship)
#Model
boost.movies.2=gbm(vote_average ~ .-vote_classes ,data=train.1,
distribution="gaussian",n.trees=5000, interaction.depth=4, shrinkage=0.1)
summary(boost.movies.2)
#Predictions
yhat.boost.2=predict(boost.movies, newdata=train.2, n.trees=1:5000)
#Errors and best number of trees
err.2 = apply(yhat.boost.2, 2, function(pred) mean((train.2$vote_average - pred)^2))
best2=which.min(err.2)
min(err.2)
best<- best2
#Final model with best number of trees on entire training set (dati.train)
boost.movies.2=gbm(vote_average ~ .-vote_classes, data=dati.train,
distribution="gaussian", n.trees=best2, interaction.depth=4)
summary(boost.movies.2)
#Prediction on test set
p.boost=predict(boost.movies.2, newdata=dati.test, n.trees=best)
dev.boost.2 <- sum((p.boost-dati.test$vote_average)^2)
#The previous model is better
dev.boost.2
library("readxl")
apple<- read_excel("DatiAPPLE.xlsx")
str(apple)
apple$iPhone
#Data cleaning (without Nan values) and cumulative data
iphone<- apple$iPhone[1:46]
iphone
iphonec<- cumsum(iphone)
iphonec
#Plots
par(mfcol=c(2,1))
plot(iphone, type="l")
plot(iphonec, type="l")
par(mfrow=c(1,1))
#Autocorrelation function
acf(iphone)
summary(iphone)
library(DIMORA)
source("DIMORA1.0.0.R")
#Bass standard (forecasting)
BMs<-BASS.standard(iphone,display = T)
BMs
#The function "fitted()" gives us a cumulative output by default, to use the "normal" fit we should use
#"make.instantaneous()" function
pred_BM<- fitted(BMs)
#Cumulative forecasting
plot(pred_BM, type="l",ylab="z(t)",xlab="t")
#Instantaneous forecasting
pred_BMinst<- make.instantaneous(pred_BM)
plot(pred_BMinst, type="l",ylab="z'(t)",xlab="t")
#Change the chacarter of labels
#plot(pred_BMinst, type="l",ylab=expression(italic("z'(t)")),xlab=expression(italic(t)))
#Cumulative data plot
plot(iphonec, xlab="Quarters", ylab="Sales", main="iPhone")
lines(pred_BM,col=2)
#Instantaneous data plot
plot(iphone, type= "l",xlab="Quarters", ylab="Sales", main="iPhone")
lines(pred_BMinst,col=2)
legend(35,20,legend= c("observed","BM"), col=c(1,2), lty=1)
#Bass Model estimates
BMs$Estimate
m<-BMs$Estimate[1,1]
m
p<-BMs$Estimate[2,1]
p
q<-BMs$Estimate[3,1]
q
#Residuals
par(mfrow=c(2,1))
#Cumulative (the residuals are not random, there is an oscillatory pattern)
plot(residuals(BMs))
#Instantaneous
plot(pred_BMinst-iphone)
#Cumulative data
acf(residuals(BMs))
#Instantaneous data (another confirmation of relationship)
acf(pred_BMinst-iphone)
par(mfrow=c(1,1))
GBMe1<-BASS.generalized(iphone, shock = "exp", nshock = 1, prelimestimates = c(m, p , q, 17, -0.1, 0.1), display=F)
GBMe1
pred_GBMe1<- fitted(GBMe1)
pred_GBMinst<-make.instantaneous(pred_GBMe1)
#Plot
plot(iphone, type= "l",xlab="Quarters", ylab="Sales", main="iPhone")
lines(pred_GBMinst,col=2)
legend(35,20,legend= c("observed","GBM-e1"), col=c(1,2), lty=1)
R2_tilde<-(GBMe1$Rsquared - BMs$Rsquared)/(1 - BMs$Rsquared)
R2_tilde
GGM <- GG.model(iphone, prlimestimates = c(m, 0.01, 0.1, p, q), display = F)
GGM
pred_GGM <- fitted(GGM)
pred_GGMinst <- make.instantaneous(pred_GGM)
#Plot comparison between GGM, BM and GBM
plot(iphone, type= "l",xlab="Quarters", ylab="Sales", main="iPhone")
lines(pred_GGMinst,col=2)
lines(pred_BMinst,col=3)
lines(pred_GBMinst, col=4)
legend(35,30,legend= c("observed","GGM", "BM", "GBM"), col=c(1,2,3,4), lty=1)
library(forecast)
#We will use the residuals obtained from GGM
residuals <- residuals(GGM)
residuals
plot(residuals, type= 'l')
acf(residuals)
arimaResiduals <- Arima(residuals, order = c(1,0,1), seasonal = list(order = c(1,0,0), period = 4))
#ARIMA model for residuals with auto.arima
#autoarima<- auto.arima(residuals)
#Best model selected with AIC (not always)
#autoarima
#ar1: coeff. autoregressive non-seasonal part
#ma1: coeff. moving average non-seasonal part
#sar1: coeff. aut. seasonal part
#mean: mean behavior
#To see the significance of our coefficients, we compute: coeff./s.e.. If the division is
#greater than two, it's significant (in this case all, except the mean, are sign.).
summary(arimaResiduals)
#Forecasting on residuals and plot comparison
Forecast <- fitted(arimaResiduals)
Forecast
plot(Forecast, lty=2)
lines(residuals, lty=2,col=2)
legend(0,20,legend= c("arima forecast", "observed t.s."), col=c(1,2), lty=2)
forecast.inst <- make.instantaneous(Forecast)
#Here we sum the predicted values using GGM and the forecasting on residuals obtained with ARIMA
forecastiPhone <- pred_GGMinst + forecast.inst
plot(forecastiPhone)
plot(iphone, type='l', xlab = "Quarters", ylab = "Sales", main = "iPhone")
lines(forecastiPhone, col=2)
lines(pred_GGMinst,col=3)
legend(35,30,legend= c("observed","Forecasting","GGM"), col=c(1,2,3), lty=1)
#Now we calculate a manual direct ARIMA and an auto direct ARIMA:
#Manual direct ARIMA (2 versions)
#Without differencing
directarima<- Arima(iphone, order = c(1,0,1), seasonal = list(order = c(1,0,0), period = 4))
summary(directarima)
NewForecast <- fitted(directarima)
NewForecast
#With differencing (d=1)
directarima_diff<- Arima(iphone, order = c(1,1,1), seasonal = list(order = c(1,0,0), period = 4))
summary(directarima_diff)
NewForecast_diff <- fitted(directarima_diff)
NewForecast_diff
#Auto direct ARIMA
autodirectarima<-  auto.arima(iphone)
summary(autodirectarima)
NewForecast2 <- fitted(autodirectarima)
NewForecast2
#Plot comparison
plot(iphone, type='l', lty=1, col=1)
lines(NewForecast, lty=1, col=2)
lines(NewForecast_diff, lty=1, col=3)
lines(NewForecast2, lty=1, col=4)
lines(forecastiPhone, col=5)
lines(pred_GGMinst,col=6)
legend(0,81,legend= c("observed t.s.", "direct manual ARIMA", "direct manual ARIMA_diff", "direct auto ARIMA", "GGM+ARIMAres" ,"GGM"), col=c(1,2,3,4,5,6), lty=1)
#Another view of results
par(mfrow=c(3,2))
plot(iphone, type='l', lty=1, col=1)
plot(NewForecast, type='l', lty=1, col=2)
plot(NewForecast_diff, type='l', lty=1, col=3)
plot(NewForecast2, type='l', lty=1, col=4)
plot(forecastiPhone, type='l', lty=1, col=5)
plot(pred_GGMinst,type='l', lty=1, col=6)
par(mfrow=c(1,1))
#Comparison between the best models: direct ARIMA with d=1 and GGM+ARIMA on resisuals
plot(iphone, type='l', lty=1, col=1)
lines(NewForecast_diff, lty=1, col=2)
lines(forecastiPhone, col=3)
legend(0,81,legend= c("observed t.s.", "direct manual ARIMA_diff", "GGM+ARIMAres"), col=c(1,2,3), lty=1)
#To compare not only with plots, we can plot the beahvior of resisuals of our two best model
#Residual of direct manual ARIMA_diff
residuals_arima_diff <- residuals(directarima_diff)
residuals_arima_diff
plot(residuals_arima_diff, type= 'l')
acf(residuals_arima_diff)
#Residual of GGM+ARIMAres
residuals_GGM_ARIMAres <- iphone - forecastiPhone
residuals_GGM_ARIMAres
plot(residuals_GGM_ARIMAres, type= 'l')
acf(residuals_GGM_ARIMAres)
#ARMAX refinement (ARIMA model with x regressor - predicted values with GGM)
arimaGGM<-Arima(iphonec, order = c(2,0,1),seasonal = list(order=c(1,0,0), period=4), xreg = pred_GGM)
arimaGGM
#Predictions
pred_sarmax<- fitted(arimaGGM)
pred_sarmaxinst<- make.instantaneous(fitted(arimaGGM))
#Plot
plot(iphone, type= "l",xlab="Quarters", ylab="Sales", main="iPhone")
lines(pred_sarmaxinst,col=2)
library("readxl")
#data from Athanaspoulos and Hyndman
library(fpp2)
#DW test
library(lmtest)
library(forecast)
#iMac example
apple<- read_excel("DatiAPPLE.xlsx")
str(apple)
imac <- apple$iMac
#Data visualization
plot(imac,type="l", xlab="quarter", ylab="iMac sales")
#Time variable "tt" for a linear model
tt<- 1:NROW(apple)
#Linear model
fitlm <- lm(imac~tt)
summary(fitlm)
#Residuals
res<- residuals(fitlm)
#Plot od residuals
#Parabolic behavior, they aren't randomly distributed
plot(res, xlab="quarter", ylab="residuals")
#Plot of the model
plot(imac,type="l", xlab="quarter", ylab="iMac sales")
abline(fitlm, col=3)
#Data transformed as time series
mac.ts<-ts(imac, frequency=4)
mac.ts
#Model with trend and seasonality
#"trend" and "seasonality" are two variables inside the tslm function
fitts <- tslm(mac.ts~ trend+season)
#Season1 is omitted because the three season (2,3,4) are calculated as the difference from
#a "base state", in this case season1. So we avoid "perfect-correlation"
summary(fitts)
rests <- residuals(fitts)
#Plot
plot(fitted(fitts))
#Durbin-Watson test
#DW value is between 0 and 4 (2 the middle value). If it is near zero it means that the residuals
#are autocorrelated (see also p-value).
dwtest(fitts)
#Plt of residuals
plot(rests, ylab="residuals")
#Plot of model
plot(mac.ts, ylab="iMac sales", xlab="Time")
lines(fitted(fitts), col=2)
acf(mac.ts)
#The two vector are obtained by trials and errors
arima<- Arima(mac.ts, order = c(0,1,1), seasonal = list(order=c(0,1,1), period=4))
arima
fitted(arima)
plot(fitted(arima))
lines(mac.ts, col=2)
legend(9,2.5,legend= c("observed","ARIMASeasonality"), col=c(2,1), lty=1)
#Forecasting (with confidence intervals)
forecast(arima)
plot(forecast(arima))
#We have white noise in residuals
residuals(arima)
acf(residuals(arima))
#auto ARIMA
#The auto.arima shows the same results
auto.arima(mac.ts)
fitted(auto.arima(mac.ts))
#Plot
#plot(auto.arima(mac.ts))
plot(fitted(auto.arima(mac.ts)))
forecast(auto.arima(mac.ts))
plot(forecast(auto.arima(mac.ts)))
a10
str(a10)
#Plot
plot(a10)
#Logaritmic transformation
plot(log(a10))
###BEST ARIMA MODEL FOR ANTIDIABETIC DRUGS EXAMPLE
#EXERCISE (FIRST TRY)
par(mfrow=c(2,1))
arima_diff_1<- Arima(a10, order=c(0,1,1))
arima_diff_2<- Arima(a10, order=c(0,2,1))
plot(fitted(arima_diff_1))
lines(a10, col=2)
plot(fitted(arima_diff_2))
lines(a10, col=2)
par(mfrow=c(1,1))
summary(arima_diff_1)
acf(residuals(arima_diff_1))
summary(arima_diff_2)
acf(residuals(arima_diff_2))
#EXERCISE (SECOND TRY)
arima_with_seas<- Arima(a10, order=c(0,1,1), seasonal=list(order=c(0,2,1), period=12))
plot(fitted(arima_with_seas))
lines(a10, col=2)
summary(arima_with_seas)
acf(residuals(arima_with_seas))
#Linear model with initial data
a0 <- 1:204
m1 <- lm(a10~a0)
summary(m1)
#Plot
par(mfrow=c(2,2))
plot(m1)
par(mfrow=c(1,1))
plot(a10)
plot(as.numeric(a10), type="l", xlab="Time", ylab="Antidiabetic drug sales")
abline(m1, col=2)
#Plot to see the projection
plot(as.numeric(a10), type="l", xlim=c(0,500), ylim=c(3,100), xlab="Time", ylab="Antidiabetic drug sales")
abline(m1, col=3)
#Log transformation
la10 <- log(a10)
plot(la10, xlab="Time", ylab="log Antidiabetic drug sales")
#Linear model with log data
m2 <- lm(la10~a0)
summary(m2)
#Plot
par(mfrow=c(2,2))
plot(m2)
par(mfrow=c(1,1))
plot(as.numeric(la10), type="l", xlab="Time", ylab="log Antidiabetic drug sales")
abline(m2, col=2)
#Plot to see the projection
plot(as.numeric(la10), type="l", xlim=c(0,500), ylim=c(1,7), xlab="Time", ylab="log Antidiabetic drug sales")
abline(m2, col=2)
#Projections of Linear model and Log Linear model comparison
plot(a10)
plot(as.numeric(a10), type="l", xlim=c(0,500), ylim=c(3,100), xlab="Time", ylab="Antidiabetic drug sales")
abline(m1, col=2)
summary(m2)
lines(1:500, exp(predict(m2, newdata=data.frame(a0=c(1:500)))), col=3)
#Exploration of m2 residuals and autocorrelation function
par(mfrow=c(3,1))
plot(residuals(m2))
plot(residuals(m2), type="l", ylab="residuals")
Acf(residuals(m2), main="ACF")
auto <- read.csv("auto-us.csv")
str(auto)
#Analysis of variable engine.size
y <- auto$city.mpg
x <- auto$engine.size
#Preliminary plot
plot(x,y,xlab="engine size", ylab="distance")
#install.packages("sm")
library(sm)
plot(x,y)
#sm.regression: nonparametric regression estimate function (performs local regression)
#h: smoothing parameter
#add=T: I can add other lines after the creation of the plot
#We can see at the beginning of our local regr. line a structure made by straight lines (we have local
#straight lines), if we want a smoother function we should add the number of points
sm.regression(x, y, h = 10, add = T)
#Increase the number of points where the function is estimated
sm.regression(x, y, h = 10, add = T, ngrid=200, col=2)
#We try with different values for h
#Increasing the value of h we have a smoother function, a lower value of h implies a jumpy function
sm.regression(x, y, h = 30, ngrid=200, col=1)
sm.regression(x, y, h = 50, add = T, ngrid=200, col=2)
sm.regression(x, y, h = 5,  add = T, ngrid=200, col=3)
sm.regression(x, y, h = 1,  add = T, col=4, ngrid=200)
#We add variability bands ('se': standar deviation)
sm.regression(x, y,   h = 30, ngrid=200, display="se")
#The bands are larger when I have few points
#Another library
#install.packages("KernSmooth")
library(KernSmooth)
plot(x, y, xlab="engine size", ylab="distance")
#plots local regression in an equivalent way (but obtain with different specifications)
a1 <- locpoly(x, y, degree=1, bandwidth=30)
lines(a1)
#We obtain the same result of: "sm.regression(x, y, h = 30, ngrid=200, col=1)"
#We may plot different polinomials: d=2 and d=3
a2 <- locpoly(x,y,degree=2,bandwidth=30)
lines(a2,col=2)
a3 <- locpoly(x,y,degree=3,bandwidth=30)
lines(a3,col=3)
# LOESS (no library required, default tool of R)
plot(x, y, xlab="engine size", ylab="distance")
#Default span= 2/3
lo1 <- loess.smooth(x,y)
lines(lo1)
#We try with other smoothing parameters 'span'
lo2 <- loess.smooth(x,y,span=0.9)
lines(lo2,col=2)
lo3 <- loess.smooth(x,y,span=0.4)
lines(lo3,col=3)
#Another way to perform loess; it performs directly the plot
#default span= 2/3, same results of "lo1 <- loess.smooth(x,y)"
scatter.smooth(x,y)
# same results of "loess.smooth(x,y,span=0.3)"
scatter.smooth(x,y, span=0.3)
#adding the 'evaluation' parameter we obtain a smoother line
scatter.smooth(x,y, span=0.3, evaluation=200)
#install.packages("splines")
library(splines)
#We select and identify the knots 'equispaced' (we will consider like "true knots" the internal ones,
#the others are "boundary knots")
xi<-seq(min(x), max(x), length=4)
xi
#Model (2 internal knots, from 2 to 3. Note: "knots=" define the positions of knots, not the number)
m1<-lm(y ~ bs(x, knots=xi[2:(length(xi)-1)], degree=3))
#For graphical reasons select 200 points where to evaluate the model
xxx<-seq(min(x),max(x),length=200)
#Make predictions by using the 'xxx' points
fit1<-predict(m1, data.frame(x=xxx))
#Plot
plot(x,y,xlab="engine size", ylab="distance")
lines(xxx,fit1,col=2)
#Vertical plots to indicate the knots
abline(v=xi[2], lty=3)
abline(v=xi[3], lty=3)
#In the same function "bs()", I may select the knots by using the degrees of freedom and the degree
#Basic functions b-spline for a cubic spline (degree=3)
#df directly related to the number of knots
#df=length(knots) + degree, so to select 2 knots we need df=5 and degrees=3;
#The knots are selected by using the quantiles of 'x' distribution
#First model with 2 internal knots (5 (degrees of freedom) - 3 (degrees) = 2 knots)
m1bis<-lm(y~bs(x, df=5, degree=3))
fit1<-predict(m1bis, data.frame(x=xxx))
lines(xxx,fit1,col=3)
#OBSERVATION
#Why do not we have the same results for m1 and m1bis? This is due to the choice of knot positions.
#m1bis considers always 2 knots
#mx: m1 model with df, knots and degree specification (mx overlaps m1)
mx<-lm(y ~ bs(x, df=5, knots=xi[2:(length(xi)-1)], degree=3))
fitx<-predict(mx, data.frame(x=xxx))
#Plot
lines(xxx,fitx,col=4)
#mx2:m1 model with df and degree specification but with 2 other internal knots (mx2 doesn't overlap m1)
mx2<-lm(y ~ bs(x, df=5, knots=c(130,260), degree=3))
fitx2<-predict(mx2, data.frame(x=xxx))
#Plot
lines(xxx,fitx2,col=5)
#Second model with no internal knots (3(df) - 3(degrees) = 0 knots)
m2 <- lm(y ~ bs(x, df=3, degree=3))
fit2<-predict(m2,data.frame(x=xxx))
lines(xxx,fit2,col=6)
#Third model with 17 knots (without specification of the knot positions)
m3<-lm(y~bs(x,df=20,degree=3))
fit3<-predict(m3,data.frame(x=xxx))
lines(xxx,fit3,col=2)
### SMOOTHING SPLINES (no library required, default tool)
plot(x,y,xlab="engine size", ylab="distance")
#Default lambda = NULL
s <- smooth.spline(x,y)
#We can plot the model to see how it describes the data: lines(s)
#Forecasting
p<- predict(s, x=xxx)
lines(p, col=1)
#We will consider different models with different values for lambda
#Model 1 (overlaps the previous model)
s1 <- smooth.spline(x,y, lambda=0.0001)
#We can plot the model to see how it describes the data: lines(s1, col=2)
#Forecasting of Model 1
p1<- predict(s1, x=xxx)
lines(p1, col=2)
#Model 2
s2 <- smooth.spline(x,y, lambda=0.00001)
#Forecasting
p2<- predict(s2, x=xxx)
lines(p2, col=3)
#Model 3
s3 <- smooth.spline(x,y, lambda=0.01)
#Forecasting
p3<- predict(s3, x=xxx)
lines(p3, col=4)
#Model 4 (lambda=1 means straight line)
s4 <- smooth.spline(x,y, lambda=1)
#Forecasting
p4<- predict(s4, x=xxx)
lines(p4, col=5)
# Model 5 (the model doesn't allow a lambda value equals to 0)
s5 <- smooth.spline(x,y, lambda=0.00000001)
#Forecasting
p5<- predict(s5, x=xxx)
lines(p5, col=6)
legend(200,50,legend= c("lambda=NULL","lambda=0.0001","lambda=0.00001","lambda=0.01","lambda=1","lambda=0.00000001"), col=c(1,2,3,4,5,6), lty=1)
#We try with other smoothing parameters like "spar" (alternative to lambda)
plot(x,y,xlab="engine size", ylab="distance")
ss1 <- smooth.spline(x,y,spar=0.8)
pp1<- predict(ss1, x=xxx)
lines(pp1, col=2)
